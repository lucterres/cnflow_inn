{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataSeismic as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "it=iter(data.train_loader)\n",
    "real_batch = next(iter(it))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[[[0.6064, 0.4235, 0.4105,  ..., 0.5496, 0.4140, 0.4903],\n",
       "           [0.6360, 0.5801, 0.3417,  ..., 0.4808, 0.4754, 0.6124],\n",
       "           [0.7092, 0.3604, 0.6128,  ..., 0.7203, 0.4393, 0.5533],\n",
       "           ...,\n",
       "           [0.3623, 0.4133, 0.7859,  ..., 0.4595, 0.5451, 0.5023],\n",
       "           [0.3419, 0.6374, 0.7905,  ..., 0.5519, 0.5540, 0.5074],\n",
       "           [0.4537, 0.8460, 0.5481,  ..., 0.5991, 0.4715, 0.4277]]],\n",
       " \n",
       " \n",
       "         [[[0.5268, 0.3711, 0.3954,  ..., 0.6562, 0.6279, 0.5548],\n",
       "           [0.7201, 0.6847, 0.4831,  ..., 0.5847, 0.5988, 0.2789],\n",
       "           [0.5225, 0.6502, 0.4690,  ..., 0.3547, 0.1949, 0.2636],\n",
       "           ...,\n",
       "           [0.4812, 0.5367, 0.5148,  ..., 0.4998, 0.5561, 0.5944],\n",
       "           [0.5852, 0.7521, 0.5984,  ..., 0.4894, 0.6851, 0.5333],\n",
       "           [0.8587, 0.6052, 0.4944,  ..., 0.6175, 0.5593, 0.6657]]],\n",
       " \n",
       " \n",
       "         [[[0.5210, 0.5910, 0.6164,  ..., 0.6241, 0.4817, 0.5561],\n",
       "           [0.4654, 0.6273, 0.6345,  ..., 0.6600, 0.7636, 0.6998],\n",
       "           [0.5353, 0.8107, 0.6274,  ..., 0.5507, 0.6668, 0.5915],\n",
       "           ...,\n",
       "           [0.7055, 0.5625, 0.6796,  ..., 0.6600, 0.5121, 0.4767],\n",
       "           [0.6323, 0.4902, 0.5171,  ..., 0.6621, 0.5035, 0.6809],\n",
       "           [0.5234, 0.4383, 0.6404,  ..., 0.7025, 0.5852, 0.6615]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[0.6874, 0.6225, 0.6807,  ..., 0.7998, 0.7853, 0.5339],\n",
       "           [0.5296, 0.6732, 0.5343,  ..., 0.8032, 0.6155, 0.5296],\n",
       "           [0.4969, 0.5693, 0.4844,  ..., 0.6330, 0.6868, 0.6068],\n",
       "           ...,\n",
       "           [0.5539, 0.7252, 0.6483,  ..., 0.7010, 0.6013, 0.4961],\n",
       "           [0.6267, 0.7002, 0.8887,  ..., 0.7151, 0.6421, 0.6665],\n",
       "           [0.5923, 0.8923, 0.4818,  ..., 0.6884, 0.7632, 0.7014]]],\n",
       " \n",
       " \n",
       "         [[[0.4995, 0.7355, 0.5265,  ..., 0.6768, 0.5409, 0.4967],\n",
       "           [0.5175, 0.6534, 0.6760,  ..., 0.7680, 0.3489, 0.4634],\n",
       "           [0.5353, 0.5509, 0.4747,  ..., 0.5538, 0.5725, 0.7150],\n",
       "           ...,\n",
       "           [0.5275, 0.5852, 0.6176,  ..., 0.6795, 0.6980, 0.5553],\n",
       "           [0.6224, 0.4359, 0.5998,  ..., 0.5490, 0.6299, 0.5105],\n",
       "           [0.6061, 0.6081, 0.4195,  ..., 0.4705, 0.6301, 0.5175]]],\n",
       " \n",
       " \n",
       "         [[[0.5200, 0.3294, 0.7168,  ..., 0.6065, 0.6091, 0.5581],\n",
       "           [0.7001, 0.5436, 0.7269,  ..., 0.5608, 0.5087, 0.3496],\n",
       "           [0.4374, 0.8685, 0.6507,  ..., 0.4898, 0.5197, 0.4027],\n",
       "           ...,\n",
       "           [0.5863, 0.3829, 0.3517,  ..., 0.3915, 0.5083, 0.4756],\n",
       "           [0.6485, 0.3807, 0.4131,  ..., 0.8127, 0.5722, 0.4372],\n",
       "           [0.4243, 0.3449, 0.4397,  ..., 0.6560, 0.4016, 0.5072]]]]),\n",
       " tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cinn = model.MNIST_cINN(5e-4)\n",
    "cinn.cuda()\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(cinn.optimizer, milestones=[20, 40], gamma=0.1)\n",
    "\n",
    "N_epochs = 60\n",
    "t_start = time()\n",
    "nll_mean = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch\tBatch/Total \tTime \tNLL train\tNLL val\tLR\n",
      "000 \t00000/00086 \t0.07 \t0.127287\t-0.148896\t5.00e-04\n",
      "000 \t00050/00086 \t0.19 \t-0.876542\t-1.474705\t5.00e-04\n",
      "001 \t00000/00086 \t0.27 \t-1.538668\t-1.675711\t5.00e-04\n",
      "001 \t00050/00086 \t0.37 \t-1.619762\t-1.944390\t5.00e-04\n",
      "002 \t00000/00086 \t0.44 \t-1.651419\t-1.966140\t5.00e-04\n",
      "002 \t00050/00086 \t0.55 \t-1.676837\t-1.976722\t5.00e-04\n",
      "003 \t00000/00086 \t0.63 \t-1.690132\t-1.996166\t5.00e-04\n",
      "003 \t00050/00086 \t0.75 \t-1.707145\t-2.038876\t5.00e-04\n",
      "004 \t00000/00086 \t0.82 \t-1.708843\t-2.020774\t5.00e-04\n",
      "004 \t00050/00086 \t0.92 \t-1.723865\t-2.024717\t5.00e-04\n",
      "005 \t00000/00086 \t0.99 \t-1.723674\t-2.063748\t5.00e-04\n",
      "005 \t00050/00086 \t1.08 \t-1.732357\t-2.046184\t5.00e-04\n",
      "006 \t00000/00086 \t1.15 \t-1.734011\t-2.053850\t5.00e-04\n",
      "006 \t00050/00086 \t1.25 \t-1.742465\t-2.057013\t5.00e-04\n",
      "007 \t00000/00086 \t1.33 \t-1.742760\t-2.067485\t5.00e-04\n",
      "007 \t00050/00086 \t1.42 \t-1.750128\t-1.998251\t5.00e-04\n",
      "008 \t00000/00086 \t1.50 \t-1.749763\t-2.033202\t5.00e-04\n",
      "008 \t00050/00086 \t1.59 \t-1.760988\t-2.046696\t5.00e-04\n",
      "009 \t00000/00086 \t1.66 \t-1.755180\t-2.055882\t5.00e-04\n",
      "009 \t00050/00086 \t1.76 \t-1.765957\t-2.046190\t5.00e-04\n",
      "010 \t00000/00086 \t1.83 \t-1.763632\t-2.071121\t5.00e-04\n",
      "010 \t00050/00086 \t1.93 \t-1.768350\t-2.062032\t5.00e-04\n",
      "011 \t00000/00086 \t2.00 \t-1.771743\t-2.080564\t5.00e-04\n",
      "011 \t00050/00086 \t2.10 \t-1.777741\t-2.061441\t5.00e-04\n",
      "012 \t00000/00086 \t2.17 \t-1.771282\t-2.074406\t5.00e-04\n",
      "012 \t00050/00086 \t2.27 \t-1.782915\t-2.066015\t5.00e-04\n",
      "013 \t00000/00086 \t2.34 \t-1.771824\t-2.062656\t5.00e-04\n",
      "013 \t00050/00086 \t2.44 \t-1.785162\t-2.077377\t5.00e-04\n",
      "014 \t00000/00086 \t2.51 \t-1.779940\t-2.066426\t5.00e-04\n",
      "014 \t00050/00086 \t2.61 \t-1.787261\t-2.078600\t5.00e-04\n",
      "015 \t00000/00086 \t2.68 \t-1.789413\t-2.083817\t5.00e-04\n",
      "015 \t00050/00086 \t2.77 \t-1.795603\t-2.076367\t5.00e-04\n",
      "016 \t00000/00086 \t2.84 \t-1.783963\t-2.076183\t5.00e-04\n",
      "016 \t00050/00086 \t2.93 \t-1.795154\t-2.081260\t5.00e-04\n",
      "017 \t00000/00086 \t3.00 \t-1.790542\t-2.077258\t5.00e-04\n",
      "017 \t00050/00086 \t3.11 \t-1.797884\t-2.084575\t5.00e-04\n",
      "018 \t00000/00086 \t3.17 \t-1.800175\t-2.087672\t5.00e-04\n",
      "018 \t00050/00086 \t3.27 \t-1.801935\t-2.069444\t5.00e-04\n",
      "019 \t00000/00086 \t3.34 \t-1.797289\t-2.073343\t5.00e-04\n",
      "019 \t00050/00086 \t3.43 \t-1.803504\t-2.063732\t5.00e-04\n",
      "020 \t00000/00086 \t3.50 \t-1.803035\t-2.073179\t5.00e-05\n",
      "020 \t00050/00086 \t3.59 \t-1.824786\t-2.100972\t5.00e-05\n",
      "021 \t00000/00086 \t3.66 \t-1.827859\t-2.106381\t5.00e-05\n",
      "021 \t00050/00086 \t3.76 \t-1.830061\t-2.104366\t5.00e-05\n",
      "022 \t00000/00086 \t3.82 \t-1.827608\t-2.108875\t5.00e-05\n",
      "022 \t00050/00086 \t3.92 \t-1.830893\t-2.108999\t5.00e-05\n",
      "023 \t00000/00086 \t3.99 \t-1.833282\t-2.112444\t5.00e-05\n",
      "023 \t00050/00086 \t4.09 \t-1.833932\t-2.110010\t5.00e-05\n",
      "024 \t00000/00086 \t4.17 \t-1.833999\t-2.109206\t5.00e-05\n",
      "024 \t00050/00086 \t4.27 \t-1.836012\t-2.114289\t5.00e-05\n",
      "025 \t00000/00086 \t4.33 \t-1.835079\t-2.112875\t5.00e-05\n",
      "025 \t00050/00086 \t4.43 \t-1.836454\t-2.110759\t5.00e-05\n",
      "026 \t00000/00086 \t4.50 \t-1.835519\t-2.109033\t5.00e-05\n",
      "026 \t00050/00086 \t4.59 \t-1.835969\t-2.113908\t5.00e-05\n",
      "027 \t00000/00086 \t4.66 \t-1.839683\t-2.110083\t5.00e-05\n",
      "027 \t00050/00086 \t4.76 \t-1.836688\t-2.114198\t5.00e-05\n",
      "028 \t00000/00086 \t4.83 \t-1.837982\t-2.113338\t5.00e-05\n",
      "028 \t00050/00086 \t4.95 \t-1.839957\t-2.115409\t5.00e-05\n",
      "029 \t00000/00086 \t5.02 \t-1.837814\t-2.116559\t5.00e-05\n",
      "029 \t00050/00086 \t5.11 \t-1.839849\t-2.113371\t5.00e-05\n",
      "030 \t00000/00086 \t5.18 \t-1.839945\t-2.112761\t5.00e-05\n",
      "030 \t00050/00086 \t5.27 \t-1.840893\t-2.115823\t5.00e-05\n",
      "031 \t00000/00086 \t5.34 \t-1.841954\t-2.121705\t5.00e-05\n",
      "031 \t00050/00086 \t5.44 \t-1.841997\t-2.118531\t5.00e-05\n",
      "032 \t00000/00086 \t5.51 \t-1.840957\t-2.116848\t5.00e-05\n",
      "032 \t00050/00086 \t5.61 \t-1.843248\t-2.116345\t5.00e-05\n",
      "033 \t00000/00086 \t5.68 \t-1.841732\t-2.117786\t5.00e-05\n",
      "033 \t00050/00086 \t5.78 \t-1.844439\t-2.116293\t5.00e-05\n",
      "034 \t00000/00086 \t5.86 \t-1.845096\t-2.120025\t5.00e-05\n",
      "034 \t00050/00086 \t5.96 \t-1.844607\t-2.121718\t5.00e-05\n",
      "035 \t00000/00086 \t6.03 \t-1.846864\t-2.121153\t5.00e-05\n",
      "035 \t00050/00086 \t6.13 \t-1.842466\t-2.118054\t5.00e-05\n",
      "036 \t00000/00086 \t6.19 \t-1.848290\t-2.116980\t5.00e-05\n",
      "036 \t00050/00086 \t6.29 \t-1.846418\t-2.115108\t5.00e-05\n",
      "037 \t00000/00086 \t6.36 \t-1.845157\t-2.117306\t5.00e-05\n",
      "037 \t00050/00086 \t6.45 \t-1.845823\t-2.122518\t5.00e-05\n",
      "038 \t00000/00086 \t6.52 \t-1.848054\t-2.121046\t5.00e-05\n",
      "038 \t00050/00086 \t6.62 \t-1.849737\t-2.119914\t5.00e-05\n",
      "039 \t00000/00086 \t6.69 \t-1.844785\t-2.122408\t5.00e-05\n",
      "039 \t00050/00086 \t6.79 \t-1.846959\t-2.122252\t5.00e-05\n",
      "040 \t00000/00086 \t6.86 \t-1.850811\t-2.119366\t5.00e-06\n",
      "040 \t00050/00086 \t6.96 \t-1.849567\t-2.122452\t5.00e-06\n",
      "041 \t00000/00086 \t7.02 \t-1.849216\t-2.124126\t5.00e-06\n",
      "041 \t00050/00086 \t7.12 \t-1.850372\t-2.123273\t5.00e-06\n",
      "042 \t00000/00086 \t7.19 \t-1.851894\t-2.124513\t5.00e-06\n",
      "042 \t00050/00086 \t7.28 \t-1.852230\t-2.124917\t5.00e-06\n",
      "043 \t00000/00086 \t7.35 \t-1.854440\t-2.125721\t5.00e-06\n",
      "043 \t00050/00086 \t7.44 \t-1.853256\t-2.125813\t5.00e-06\n",
      "044 \t00000/00086 \t7.51 \t-1.849515\t-2.125835\t5.00e-06\n",
      "044 \t00050/00086 \t7.61 \t-1.852670\t-2.124936\t5.00e-06\n",
      "045 \t00000/00086 \t7.67 \t-1.852774\t-2.126553\t5.00e-06\n",
      "045 \t00050/00086 \t7.77 \t-1.851809\t-2.125313\t5.00e-06\n",
      "046 \t00000/00086 \t7.84 \t-1.854074\t-2.126623\t5.00e-06\n",
      "046 \t00050/00086 \t7.93 \t-1.852731\t-2.125417\t5.00e-06\n",
      "047 \t00000/00086 \t8.00 \t-1.852901\t-2.126230\t5.00e-06\n",
      "047 \t00050/00086 \t8.10 \t-1.850013\t-2.125040\t5.00e-06\n",
      "048 \t00000/00086 \t8.16 \t-1.854926\t-2.125849\t5.00e-06\n",
      "048 \t00050/00086 \t8.26 \t-1.853211\t-2.126628\t5.00e-06\n",
      "049 \t00000/00086 \t8.32 \t-1.851437\t-2.125798\t5.00e-06\n",
      "049 \t00050/00086 \t8.42 \t-1.851356\t-2.125228\t5.00e-06\n",
      "050 \t00000/00086 \t8.49 \t-1.855680\t-2.126162\t5.00e-06\n",
      "050 \t00050/00086 \t8.59 \t-1.852576\t-2.126822\t5.00e-06\n",
      "051 \t00000/00086 \t8.66 \t-1.852325\t-2.124605\t5.00e-06\n",
      "051 \t00050/00086 \t8.75 \t-1.853791\t-2.125415\t5.00e-06\n",
      "052 \t00000/00086 \t8.82 \t-1.852587\t-2.126099\t5.00e-06\n",
      "052 \t00050/00086 \t8.92 \t-1.853069\t-2.126752\t5.00e-06\n",
      "053 \t00000/00086 \t8.99 \t-1.855107\t-2.126687\t5.00e-06\n",
      "053 \t00050/00086 \t9.08 \t-1.853512\t-2.126768\t5.00e-06\n",
      "054 \t00000/00086 \t9.15 \t-1.854327\t-2.126108\t5.00e-06\n",
      "054 \t00050/00086 \t9.24 \t-1.853652\t-2.126406\t5.00e-06\n",
      "055 \t00000/00086 \t9.31 \t-1.853665\t-2.126696\t5.00e-06\n",
      "055 \t00050/00086 \t9.41 \t-1.854234\t-2.126775\t5.00e-06\n",
      "056 \t00000/00086 \t9.47 \t-1.853589\t-2.126892\t5.00e-06\n",
      "056 \t00050/00086 \t9.57 \t-1.854166\t-2.126482\t5.00e-06\n",
      "057 \t00000/00086 \t9.64 \t-1.854311\t-2.126136\t5.00e-06\n",
      "057 \t00050/00086 \t9.73 \t-1.854140\t-2.126686\t5.00e-06\n",
      "058 \t00000/00086 \t9.80 \t-1.855171\t-2.126635\t5.00e-06\n",
      "058 \t00050/00086 \t9.89 \t-1.856182\t-2.128852\t5.00e-06\n",
      "059 \t00000/00086 \t9.96 \t-1.851450\t-2.127161\t5.00e-06\n",
      "059 \t00050/00086 \t10.06 \t-1.852590\t-2.127097\t5.00e-06\n"
     ]
    }
   ],
   "source": [
    "print('Epoch\\tBatch/Total \\tTime \\tNLL train\\tNLL val\\tLR')\n",
    "for epoch in range(N_epochs):\n",
    "    for i, (x, l) in enumerate(data.train_loader): #data.train_loader):\n",
    "        x, l = x.cuda(), l.cuda()\n",
    "        z, log_j = cinn(x, l)\n",
    "\n",
    "        nll = torch.mean(z**2) / 2 - torch.mean(log_j) / model.ndim_total\n",
    "        nll.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cinn.trainable_parameters, 10.)\n",
    "        nll_mean.append(nll.item())\n",
    "        cinn.optimizer.step()\n",
    "        cinn.optimizer.zero_grad()\n",
    "\n",
    "        if not i % 50:\n",
    "            with torch.no_grad():\n",
    "                z, log_j = cinn(data.val_x, data.val_l) #cinn(data.val_x, data.val_l)\n",
    "                nll_val = torch.mean(z**2) / 2 - torch.mean(log_j) / model.ndim_total\n",
    "\n",
    "            print('%.3i \\t%.5i/%.5i \\t%.2f \\t%.6f\\t%.6f\\t%.2e' % (epoch,\n",
    "                                                            i, len(data.train_loader),\n",
    "                                                            (time() - t_start)/60.,\n",
    "                                                            np.mean(nll_mean),\n",
    "                                                            nll_val.item(),\n",
    "                                                            cinn.optimizer.param_groups[0]['lr'],\n",
    "                                                            ), flush=True)\n",
    "            nll_mean = []\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(cinn.state_dict(), 'output/mnist_cinn.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6519ec3d6d74aa3a8a2f9552931a87abcc4f0825d19a87e9d6be02805cd5a20b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
